## R script for SAARA (Statistical Analysis of Acetylene Reduction Assay)
## To be embedded in a C++ overlay
##
## Autor : Quentin Nicoud
## Date of : 09.05.2019
## R version : 3.5.2
## List of needed packages :
##      - readxl 1.3.1
##      - xlsx 0.6.1
##      - car 3.0-2
##      - testit 0.9
##      - RInside 0.2.15
##
## Layout :
##      1 - Set up initial parameters
##      2 - Extract data from the tree generated by the Agilent software
##      3 - Transform raw data into processed data
##              i.e. pA.s-1 into mmolC2H4
##              An output table can be generated at this point
##      4 - If asked by  user the program can go further and propose to make the statistical analysis
##      5 - At last, a gaphical preview of the dat can be displayed (not sure if it will be implemented)

## TO DO LIST :
#
#   Remark : For the embedding to options are available. One is to embed the R script into Qt using RInside.
#                   The other is to build the Qt with R using the bioconductor package qtbase (found out that is not compatible with this R version)
#   In C++
#       - code the GUI using Qt
#       - add a save config function that generates an .xml or .config or .ini ? 
#
#   In R
#       - Re-write the code on data statistical analysis
#       - Add varaible neccessary to the communication between R and C++
#       - Add assertion and try
#       - include tests on the number of arguments given to each fnctions usin 'nargs'
#       - Find a way to maintain versions of packages and to transmit R with the application. --> devtools package
#       - add the nodule mass and/or weight and the vial volume in the template to be used in the different functions.
#       - template is set up with no header. should that be added for an esaier usage of the program ?
#       - remove package mannagement from all function as I added load_SAARA_packages an unload_SAARA_packages

rm(list = ls())

#install.packages('openxlsx', 'xlsx', 'gdata', 'readxl', 'RInside')
library(RInside)


# --------------------------------------------------------------------------------------------------------------------------------------------------------------
                ########################################################################################################################
                ##                                  CODE RELATED TO THE FUNCTIONNING OF THE PIPELINE                                  ##
                ########################################################################################################################

# Varaible that should recieve input from the C++ programm
normalityThreshold <- 0.05
varHThreshold <- 0.05
poolData <- FALSE ##TRUE = testing several mutants OR repetition ##FALSE = kinetics
isKinetics <- FALSE
splitV <- 5
custom_formula <- 0
slope <- 495

list_of_required_pckg <- data.frame(pckg = c("readxl", "xlsx", "car", "testit", "RInside"), 
                                    version = c("1.3.1", "0.6.1", "3.0-2", "0.9", "0.2.15"), stringsAsFactors = FALSE)

load_SAARA_packages <- function(list_of_required_pckg)
{

    ## Install and/or load packages required in the analysis.
    ## First, it tries to use devtools to install packages versions that were used duing the developpement of this script.
    ## Then it checks if every pacakges are indeed loaded, if not anyLib will install the last verion.
    ##
    ## Usage :
    ##          output <- load_SAARA_packages(list_of_required_ pckg)
    ##
    ## Intput :
    ##      - list_of_required_pckg:    a list of two component. Fisrt the list of packages to be installed and then the list of versions 
    ##                                  in the same order as the list of packages that specifies in which version each package should be installed.
    ##
    ## Output :
    ##      - are_pckg_loaded:          a vector containing logical values that specifies if each package could be loaded. Values are in the same order
    ##                                  than in the list_of_required_pckg.
    
    
    # If needed install devtools, otherwise, load it. Required for installing packages with a specific version (install_version function)
    if(!require(devtools))
    {
        install.packages("devtools", dependencies = TRUE)
        
        library(devtools)
    }
    
    # If needed install installR, otherwise load it. Required for installing Rtools which is needed to compile packages from source (install_version function)
    if(!require(installr))
    {
        install.packages("installr", dependencies = TRUE)
        
        library(installr)
    }
    install.rtools()
    
    # Run through every required package
    for ( i in 1:dim(list_of_required_pckg)[1])
    {
        # If required, install it at the right version
        if (!require(list_of_required_pckg$pckg[i]))
        {
            devtools::install_version(list_of_required_pckg$pckg[i], version = list_of_required_pckg$version[i], upgrade = "never")
            
            library(list_of_required_pckg$pckg[i])
        }
    }
    
    # Then, use anylib to check the installation was successfull. If not it will install the newest version.
    if(!require(anyLib))
    {
        install.packages("anyLib", dependencies = TRUE)
        
        library(anyLib)
    }
    #Store the vector containing a booleean value that specifies if the package is loaded or not.
    are_pckgs_loaded <- anyLib::anyLib(list_of_required_pckg[[1]])
    # Return this value
    are_pckgs_loaded
    
   # if(!require(devtools))
   # {
   #     install.packages("devtools", dependencies = TRUE)
   #     
   #     library(devtools)
   # }
   # 
   #  # for ( i in 1:dim(list_of_required_pckg)[1])
   #  # {
   #  #     if (!require(list_of_required_pckg$pckg[i]))
   #  #     {
   #  #         devtools::install_version(list_of_required_pckg$pckg[i], version = list_of_required_pckg$version[i], upgrade = "never")
   #  #         
   #  #         library(list_of_required_pckg$pckg[i])
   #  #     }
   #  # }
   #  
   #  new_pckg <- list_of_required_pckg$pckg[!(list_of_required_pckg$pckg %in% installed.packages()[, "Package"])]
   #  if (length(new_pckg))
   #  {
   #      new_pckg_ids <- which(list_of_required_pckg$pckg %in% new_pckg)
   #      for (i in 1:length(new_pckg))
   #      {
   #          tryCatch( expr = { 
   #              devtools::install_version(new_pckg[i], list_of_required_pckg$version[new_pckg_ids[i]], dependencies = TRUE)
   #              library(new_pckg[i])},
   #              
   #                    error = function(err) {
   #                      #Warning handler
   #                      print(paste("ERROR_CAUGHT: ", err))
   #                      next}
   #              
   #                    finally = {install.packages(new_pckg[i])}
   #          )
   #      }
   #  }
    
} # To be tested

unload_SAARA_packages<- function(list_of_required_pckg)
{
    for ( i in 1:dim(list_of_required_pckg)[1])
    {
        detach(paste("package", list_of_required_pckg$pckg[i], sep = ""), unload = TRUE)
    }
    
    detach("package:devtools", unload = TRUE)
} # To be tested

# --------------------------------------------------------------------------------------------------------------------------------------------------------------
                ########################################################################################################################
                ##                                FUNCTIONS RELATED TO DATA EXTRACTION AND CALCULATIONS                               ##
                ########################################################################################################################

trim_file_path <- function(file_path)
{
    ## In a given string containing the path of a file or folder, 
    ## will trim the string ton anly keep the folder o file name. 
    ##
    ## Usage:
    ##          output <- trim_file_path(file_path)
    ##
    ## Input:
    ##      - file_path :       a string containing the path of the given file/folder
    ##
    ## Output:
    ##      - trimmed_string :  the string containning only the folder/file name. 
    
    if (nargs <= 1)
    {
        
    }
    
    
    trimmed_string <- unlist(strsplit(file_path, '/'))[length(unlist(strsplit(file_path, '/')))]
    
    trimmed_string
} ## Ok<- not needed anymore


calculate_nmolC2H4_H_Plant <- function(pA_s, delta_time = 120, slope = 495, vial_volume = 21, splitV = 5, 
                                       customFormula = 0, nodule_weight = c(), nodule_number = c())
{
    ## Function that calculates the nitrogenase activity in nmol of ethylene produced per hour and per plant
    ##
    ## Usage:
    ##          output <- calculate_nmolC2H4_H_Plant(pA_s, delta_time = 120, slope = 495, vial_volume = 21, splitV = 5, 
    ##                                               customFormula = 0, nodule_weight = c(), nodule_number = c())
    ##
    ## The default formula is:
    ##          nmolC2H4_H_Plant <- ( pA_s / delta_time ) * (vial_volume / slope) * 60 * splitV
    ## Input:
    ##      - pA_s :            raw data from the machine (pA.s-1)
    ##
    ##      - delta_time :      the difference between time when the sample is injected into the column and the time when sample is incubated with 
    ##                          acetylene (in minutes)
    ##
    ##      - slope :           OPTIONNAL, slope of the standard curve, allows to correlated mol of produced ethylene to the pA.s-1
    ##                          the standard curve should be established by injecting known quantities of ethylene into the GC-FID (default 495)
    ##
    ##      - vial volume :     OPTIONNAL, used in the calculations, can be modified if the experimentator measures the remaining volume in the vials
    ##                          this is generally used when plants occupy a big part of the vial volume (in mL ; default 21)
    ##
    ##      - splitV :           OPTIONNAL, specifies the value of the split factor. Necessary if the GC-FID method is using the split parameter.
    ##                          This parameter is used to reduce the quantity of sample injected in the column to avoid saturation.
    ##                          This script was designed to fit the method used in the GC-FID. In our case, the headspace fees 1 mL of the sample.
    ##                          If the split option is activated, then only a fifth of this 1 mL will be injected in the column (so 200 µL)
    ##                          To use this script with method that were designed differently, the value of split can be changed to fit the real 
    ##                          volume of sample injected.
    ##
    ##      - customFormula :   OPTIONNAL, a formula used to make the calculations. This option should be used carrefully as it can esaly lead to errors.
    ##                          Default is set to 0. This value is used as in a test for the programm to use the default formula.
    ##
    ## Outout:
    ##      - nmolC2H4_H_Plant: a double variable containing the processed value.

        # The following test allows to change the formula if the user wants to.
    if (customFormula == 0)
    {
        formula <- "(( pA_s / delta_time ) * (vial_volume / slope) * 60 * splitV)"
        
        if (length(nodule_number != 0) & length(nodule_weight != 0))
        {
            #error
            print("ERROR: Both nodules number and nodule weight where given. Will not normalise.")
        }
        else
        {
            if (length(nodule_number != 0))
            {
                formula <- paste(formula, "/ ", nodule_number)
            }  
            if (length(nodule_weight != 0))
            {
                formula <- paste(formula, "/ ", nodule_weight)
            }
        }
    }
    else
    {
        formula <- customFormula
    }

        # Evaluate the expression
    nmolC2H4_H_Plant <- eval(parse(text = formula))

        # Return the output nmolC2H4_H_Plant
    nmolC2H4_H_Plant
} ## Ok <- error assertion has to be improved


calculation <- function(extracted_data, slope = 495, vial_volume = 21, splitV = 5, custom_formula = 0)
{
    ## Function that format data and stores converts the raw values into the processed data using calculate_nmolC2H4_H_Plant
    ##
    ## Usage:
    ##          output <- calculation(extracted_data, slope, vial_volume, splitV, customFormula)
    ##
    ## Input:
    ##      - extracted_data :  a data frame that contains the following data, in the same order:
    ##          - condition name (a string)
    ##          - sample ID, within a condition (integers)
    ##          - time_start_incubation : time when acetylene is infected into vials (hours & minutes on a different element of the data.frame))
    ##          - injection_ime : time when the sample is injected into the column (hours & minutes on a different element of the data.frame))
    ##          - pA_s : raw data from the machine (pA.s-1)
    ##          - nodule number : optionnal data that can be added to the analysis if needed (integers)
    ##          - nodule weight :  optionnal data that can be added to the analysis if needed (in mg)
    ##
    ##
    ##      - slope :           Slope of the standard curve, allows to correlated mol of produced ethylene to the pA.s-1
    ##                          the standard curve should be established by injecting known quantities of ethylene into the GC-FID (default 495)
    ##
    ##      - vial_volume :     Used in the calculations, can be modified if the experimentator measures the remaining volume in the vials
    ##                          this is generally used when plants occupy a big part of the vial volume (in mL ; default 21)
    ##
    ##      - splitV :           Specifies the value of the split factor. Necessary if the GC-FID method is using the split parameter.
    ##                          This parameter is used to reduce the quantity of sample injected in the column to avoid saturation.
    ##                          This script was designed to fit the method used in the GC-FID. In our case, the headspace fees 1 mL of the sample.
    ##                          If the split option is activates, then only a fifth of this 1 mL will be injected in the column (so 200 µL)
    ##                          To use this script with method that were designed differently, the value of split can be changed to fit the real 
    ##                          volume of sample injected.
    ##
    ##      - custom_formula :   OPTIONNAL, a formula used to make the calculations. This option should be used carrefully as it can esaly lead to errors.
    ##                          Default is set to 0. This value is used as in a test for the programm to use the default formula.
    ##
    ## Output:
    ##      - list_of_values :  a data.frame that contains :
    ##                              - condition_name (see the input extracted_data)
    ##                              - sample_id (see the input extracted_data)
    ##                              - nmolC2H4_H_Plant: the processed data.
    
    
        # Create a data.frame that will contain the processed data
    list_of_values <- data.frame(condition_name = extracted_data[,1], sample_id = extracted_data[,2], 
                                 nmolC2H4_H_plant = rep(0, times = dim(extracted_data)[1]))

        # Loop that will go through every sample in the extracted_data data.frame and use the function calculate_nmolC2H4_h_plant to process the data.
    for (i in 1:dim(extracted_data)[1])
    {
        delta_time <- extracted_data[[i,6]]*60 + extracted_data[[i,5]] - extracted_data[[i,4]]*60 + extracted_data[[i,3]]
        list_of_values[i,3] = calculate_nmolC2H4_H_Plant(pA_s = extracted_data[i,7], delta_time, slope, vial_volume, splitV, custom_formula, nodule_weight = c(), nodule_number = c())
    }
        
        # Return the list_of_values
    list_of_values
} ## Ok <- nodule nbr/mass have to be implemented as well as vial_volume + must improve output


get_tree_path <- function(pathToExpeFolder)
{
    ## Function that extract the path of each subfolder of every experiments
    ##
    ## Usage:
    ##          output <- get_tree_path(pathToxpeFolder)
    ##
    ## Input:
    ##      - pathToExpeFolder :    the name of the folder containing the data
    ##
    ## Output:
    ##      -sample_folders :       a data.frame containing the name of the folders of each samples in every experiment. 
    ##                              Each column of the data.frame is named after the experience folder.

    
        # List all experiment folders.
    expe_folder <- list.dirs(pathToExpeFolder, recursive = FALSE, full.names = FALSE)
        
        # Create the data.frame that will contain all the sample names of all experiments in the expeFolder.
    sample_folders <- lapply(expe_folder, function(x) 0)
    names(sample_folders) <- expe_folder
        
        # Loop that goes through expe_folder to recover all the names of sample folders.
    for (i in 1:length(expe_folder))
    {
            # Get sample path
        sample_folders[[i]] <- list.dirs(paste(pathToExpeFolder, '/', expe_folder[[i]], sep = ""), recursive = FALSE, full.names = FALSE)
            # Removes the method folder.
        sample_folders[[i]] = sample_folders[[i]][-grep(".M", sample_folders[[i]])]
    }
    
        # Return output
    sample_folders
} ## Ok <- add strmatch to rmv unwanted folders and other stuff. (done I think)


data_extraction <- function(pathToExpeFolder)
{
    ## Function that extract the data from the tree.
    ## Based on the position of each peaks in our assays. May be different for other experimental designs (this function can be improved).
    ## The peak that holds the highest value in measured retention time corresponds to acetylene in our experiemental design.
    ## It is preceeded by the ethylene peak and before that a 'contamination' peak (alaways present).
    ##
    ## Usage:
    ##          output <- data_extraction(pathToExpeFolder)
    ##
    ## Input:
    ##      - pathToExpeFolder :    the path to the folder containing the data
    ##
    ## Output:
    ##      - all_data :            a data.frame that contains all the required data.
    ##                                  - subfile_name : the name of the path from which the data was extracted
    ##                                  - sample_id : the name of the sample as given by the user before the run. 
    ##                                  - injection_time_H : hour of sample injection in the GC.
    ##                                  - injcetion_time_min : minutes of sample injection in the GC.
    ##                                  - ethylene_MeasRetTime : measured retention time.
    ##                                  - area : values of pA_s-1 measured by the FID.
    
    
        # Load readxl, and if not installed, install it.
    if (!require(readxl))
    {
        install.packages("readxl")
        library(readxl)
    }
    
        # Get paths thanks to the get_tree_path function
    paths <- get_tree_path(pathToExpeFolder)
    
        # Prepare the list all_data
    all_data <- lapply(paths, function(x) 0)
    
        # For loop that goes through each experimment folders
    for (i in names(paths))
    {
            # Prepare a temporary data holder
        data <- data.frame(subfile_name=0, sample_id=0, injection_time_H=0, 
                           injection_time_min=0, ethylene_MeasRetTime=0, area=0)
        
            # For loop that goes through each sample folder
        for (j in paths[[i]])
        {
            temp_peak_data <- read_xls(paste(pathToExpeFolder, i, j, "REPORT01.xls", sep = '/'), sheet = "Peak")[,1:15] # Read data
            temp_peak_data[is.na(temp_peak_data)] <- 0 # Remove Na and replace them by zeros.
            
                # Identify the peak that holds the ethylene value, based on the measured retention time.
            ratioRetTime <- temp_peak_data$MeasRetTime/max(temp_peak_data$MeasRetTime) # This makes a ratio between values of each 
                                                                                        # peak and the max value of these values.
            pos <- which(ratioRetTime == 1)-1   # Get only the value before the acetylene
            while (temp_peak_data$MeasRetTime[pos] == 0) # Sometimes the predicted elution time of the different 
                                                         # compounds interfers with the measured elution time. To avoid that,
                                                         # this loop searches for the first value in the MeasRetTime that is different to zero.
            {
                pos = pos - 1
                if (pos == 1)
                {
                    ## error
                    break
                }
            }
            
            temp_base_info <- read_xls(paste(pathToExpeFolder, i, j, "REPORT01.xls", sep = '/'), sheet = "Sheet1") # Read info data
            
            data <- rbind(data, c(paste(i, j, sep="/"), as.character(temp_base_info[temp_base_info[,1]=="SampleInfo",2]),
                              substr(temp_base_info[temp_base_info[,1]=="InjDateTime",2], start = 12, stop = 13),
                              substr(temp_base_info[temp_base_info[,1]=="InjDateTime",2], start = 15, stop = 16),
                              temp_peak_data$MeasRetTime[pos], temp_peak_data$Area[pos])) # Store everything into one data.frame
                    }
        data <- data[2:dim(data)[1],] # Trim the first row of the data.frame which only contains zeros
        all_data[[i]] <- data # Store the data frame in the all_data list.
    }
    
        # Detach package
    detach("package:readxl", unload = TRUE)
    
        # Return all_data
    all_data
} ## Ok


template_gen <- function(pathToExpeFolder, path_to_template)
{
    ## Function that formates the template data given as an excel file.
    ## The given file must be an .xls or .xlsx and must be present in the folder
    ## which path is given by pathToExpeFolder. The tables must not have headers.
    ## This file must carry one sheet per experiment, detailling for each sample the necessary data:
    ##              - The condition identifier used in the Agilent Software during the Easy Queue setting.
    ##              - condition_name : the identifier of the condition (name of the mutant, of the treatment, etc.)
    ##              - sample_number : the number of this sample within a given condition.
    ##              - incubation_hour : time when acetylene was injected in the vials (hours).
    ##              - incubation_minutes : time when acetylene was injected in the vials (minutes).
    ##                      For example, if the incubation of the first sample began at 14:35,
    ##                      incubation_hour of the first line would be 14 and incubation_minutes would be 35.
    ##                  This for all experiments
    ##
    ## Usage:
    ##          output <- template_gen(pathToExpeFolder, path_to_template)
    ##
    ## Inputs:
    ##      - pathToExpeFolder : the path to expe folders
    ##
    ##      - path_to_template : the path to find the excel file
    ##
    ## Output:
    ##      - template : data.frame aving the same composition as the excel file
    
    
        # Load readxl, and if not installed, install it.
    if (!require(readxl))
    {
        install.packages("readxl")
        library(readxl)
    }
    
        # Generate the template list
    expe_names <- list.dirs(pathToExpeFolder, full.names = FALSE, recursive = FALSE)
    template <- lapply(expe_names, function(x) 0)
    names(template) <- expe_names
        
        # Loop that goes through each of the sheets of the template excel file and stores it into the template list
    for (i in 1:length(template))
    {
        template[[i]] <- read_excel(path_to_template, sheet = i, col_names = FALSE)
    }
    
        # Detach package
    detach("package:readxl", unload = TRUE)
    
        # Return template
    template
} ## Ok


data_formating_and_calc <- function(all_data, template)
{
    ## Takes the all_data data.frame generated from the data_extraction function.
    ## Returns processed extracted_data which will be used for the calculations.
    ## Calls 
    ##
    ## Usage:
    ##          output <- data_formating_and_calc(all_data, template)
    ##
    ## Inputs:
    ##      - all_data : data.frame containing output data from the data_extraction function
    ##
    ##      - template : a data.frame carring:
    ##              - The condition identifier used in the Agilent Software during the Easy Queue setting.
    ##              - condition_name :      the identifier of the condition (name of the mutant, of the treatment, etc.)
    ##              - sample_number :       the number of this sample within a given condition.
    ##              - incubation_hour :     time when acetylene was injected in the vials (hours).
    ##              - incubation_minutes :  time when acetylene was injected in the vials (minutes).
    ##                      For example, if the incubation of the first sample began at 14:35,
    ##                      incubation_hour of the first line would be 14 and incubation_minutes would be 35.
    ##              - vial_volume :         the volume of the vial in mL
    ##                  This for all experiments
    ##
    ## Output:
    ##      - result_data : a list that carries, in each of its elements (experiences) a data.frame that contains:
    ##              - condition_name
    ##              - sample_number
    ##              - nmolC2H4_H_Plant : values of processed data.
    
    
        # Generate the output data.frame
    result_data <- lapply(names(all_data), function(x) 0)
    names(result_data) <- names(all_data)
    
        # For loop that goes through each experiment
    for (i in names(all_data))
    {
            # generate the data.frame that will hold the formated data for the calculation
        extracted_data <- data.frame(condition_name = as.character(rep("", times = dim(all_data[[i]])[1])), 
                                     sample_number = rep(0, times = dim(all_data[[i]])[1]), 
                                     incubation_hour = rep(0, times = dim(all_data[[i]])[1]), 
                                     incubation_minutes = rep(0, times = dim(all_data[[i]])[1]), 
                                     injection_hour = as.numeric(unlist(all_data[[i]]["injection_time_H"])), 
                                     injection_minutes = as.numeric(unlist(all_data[[i]]["injection_time_min"])), 
                                     pA_s = as.numeric(unlist(all_data[[i]]["area"])), nodule_number = 0, nodule_weight = 0, 
                                     stringsAsFactors = FALSE)
        
            # Recover data from the template data and assign it to the correct row in extraced_data
        for (j in 1:dim(template[[i]])[1])
        {
                # Get the indice of the sample_id that correspond to the actual row in template
            pos <- which(all_data[[i]]["sample_id"] == unlist(template[[i]][j,1]))
            
                # Assign data to extracted_data
            extracted_data[pos, "condition_name"] <- unlist(template[[i]][j,2])
            extracted_data[pos, "sample_number"] <- unlist(template[[i]][j,3])
            extracted_data[pos, "incubation_hour"] <- as.numeric(unlist(template[[i]][j,4]))
            extracted_data[pos, "incubation_minutes"] <- as.numeric(unlist(template[[i]][j,5]))
        }
        
        
        result_data[[i]] <- calculation(extracted_data, slope = 495, vial_volume = 21, splitV = 5, custom_formula = 0)
    }   
    
        # Return extracted_data
    result_data
} ## Ok add vial_volume in the pipeline as well as nodule mass


write_data <- function(result_data, save_path)
{
    ## Save the results into one excel file. Maybe not usefull, depends on how much the Qt App can be linked with this script.
    ##
    ## Usage:
    ##          output <- write_data(result_data, save_path)
    ##
    ## Input:
    ##      - result_data : list of data.frame generated by the data_formating_and_calc function.
    ##
    ##      - save_path : the path to the saved file.
    ##
    ## Output:
    ##      - none : this function should only save the data and return nothing
    
    
    # Load xlsx, and if not installed, install it.
    if (!require(xlsx))
    {
        install.packages("xlsx")
        library(xlsx)
    }
    
        # Check the presence of the file
    if (!file.exists(save_path))
    {
        for (i in names(result_data))
        {
            write.xlsx(result_data[[i]], save_path, sheetName = i, append = TRUE)
        }    
    }
    else
    {
        print("File already exist.")
    }
    
    detach("package:xlsx", unload = TRUE)
    
} ## Ok <- overwrite to be reviewed


remove_controls <- function(template)
{
    for (i in 1:length(names(template)))
    {
        template[which(template[[i]][,2] == "control"),1]
        
    }
} ## Ongoing

pool_expe <- function(results)
{
    
}

#---------------------------------------------------------------------------------------------------------------------------------------------------------------
                ########################################################################################################################
                ##                                FUNCTIONS RELATED TO DATA STATISTICAL ANALYSIS                                      ##
                ########################################################################################################################

check_normality <- function(result, normalityThreshold = 0.05)
{
    ## Testing for normality to perform FTest
    ##
    ## Usage:
    ##          output <- check_normality(result, normalityThreshold)
    ##
    ## Input:
    ##      - result :              the list generated by the remove_controls function
    ##
    ##      - normalityThreshold :  a single integer value that gives the threshold for the false 
    ##                              positive rate or probability of type I error (default 0.05).
    ##
    ## Output:
    ##      - test_result :         a list which length equals the number of experiments. For each of its elements, this list contains a list of two vectors:
    ##              - p_values :        the p.value of the test for each condition.
    ##
    ##              - boolean_result :  the result of the test transformed into boolean values.
    ##                                  Based on the threshold given as second input.
    
    
    expeNbr <- length(names(result))
    
    s_w_results <- lapply(result, function(x) 0)
        # Loop that goes through every experiment
    
    for (i in 1:expeNbr)
    {
        
        condNames <- levels(as.factor(result[[i]][,"condition_name"]))
        
        testSW <- rep(0, times = length(condNames))
        isNormal <- rep(0, times = length(condNames))
        
        names(testSW) <- names(isNormal) <- condNames
        
            # Loop that goes through every conditions of the current experiment.
        for (j in 1:length(condNames))
        { ##scan conditions to determine normality of each data subset
            indices <- grep(condNames[j], result[[i]][,"condition_name"], fixed = TRUE)
            
                # If the size of the sample is smaller that 3, the the test cannot be performed.
                # The value of 2 is assigned to this condition as it is an impossible value of the p-value.
                # Used for testing later-on, and  to distinguished these conditions from non-normal samples.
            if (length(indices) < 3)
                testSW[j] <- 2
            
                # Otherwise, perform the satistical test.
            else
                testSW[j] <- shapiro.test(result[[i]][indices, "nmolC2H4_H_plant"])$p.value
            
                # Assign a boolean value depending on the result of the test.
            if (testSW[j] >= normalityThreshold)
                isNormal[j] <- TRUE
            
            else 
                isNormal[j] <- FALSE
            
                # NA is for samples which have been associated with the value 2.
            if (testSW[j] == 2)
                isNormal[j] <- NA
        } 
        s_w_results[[i]] <- list(p_values = testSW, boolean_result = isNormal)
    }
    
        # Return s_w_results
    s_w_results
} ## Ok

check_var_h <- function(result, varHThreshold = 0.05)
{
    ## F-test : Testing variance homescedasticity
    ##
    ## Usage:
    ##          output <- check_var_h(result, varHThreshold = 0.05)
    ##
    ## Inputs:
    ##      - result :          the list as generated by the remove_controls function
    ##
    ##      - varHThreshold :   a single integer value that gives the threshold for the false 
    ##                          positive rate or probability of type I error (default 0.05).
    ##
    ## Outut:
    ##      - varH :            a list for which each element is related to an experiment. For each of its elements, 
    ##♣                         this list contains a list of two matrixes:
    ##              - p_values :        the p.value of the test for each combination of condition.
    ##
    ##              - boolean_result :  the result of the test transformed into boolean values.
    ##                                  Based on the threshold given as second input.
    
    expeNbr <- length(names(result))
    
    varH <- lapply(result, function(x) 0)
    
    for (i in 1:expeNbr) 
    { ##Loop for experiments scannin
        condNames <- levels(as.factor(result[[i]][,"condition_name"]))
        
        ## A CAHNGER FAIRE DES MATRICES a stocker dans un data Frame
        temp_val <- list(p_values = 0, boolean_results = 0)
                               
        p_values <- matrix(NA, nrow = length(condNames), ncol = length(condNames))
        bool_vals <- matrix(NA, nrow = length(condNames), ncol = length(condNames))
        rownames(p_values) <- rownames(bool_vals) <- colnames(p_values) <- colnames(bool_vals) <- condNames
        
        all_comb <- do.call(c, lapply(seq_along(condNames), combn, x = condNames, simplify = FALSE))
        
            # Go though every possible combination
        for (j in all_comb)
        {
                # Remove the combination of elements that involve a number of element different from 2
            if (length(j) == 2)
            {
                    # Verify that the combination has not been tested yet
                if (is.na(p_values[j[1], j[2]]) && is.na(p_values[j[2], j[1]]))
                {
                    if (length(result[[i]][j[1],"nmolC2H4_H_plant"]) > 3 && length(result[[i]][j[2],"nmolC2H4_H_plant"]) > 3)
                    {
                        p_values[j[1], j[2]] <- var.test(result[[i]][j[1],"nmolC2H4_H_plant"], 
                                                         result[[i]][j[2],"nmolC2H4_H_plant"])$p.value
                    }
                    else
                        p_values[j[1], j[2]] <- 2
                    
                    ## Then tests bool
                    if (p_values[j[1], j[2]] >= varHThreshold)
                        bool_vals[j[1], j[2]] <- TRUE
                    
                    else 
                        bool_vals[j[1], j[2]] <- FALSE
                    
                    if (p_values[j[1], j[2]] == 2)
                        bool_vals[j[1], j[2]] <- NA
                }
            }
        }
        temp_val[[1]] <- p_values
        temp_val[[2]] <- bool_vals
        
        varH[[i]] <- temp_val
    }
     
    varH
} ## Ok

check_means <- function(result, normality_results, var_h_results)
{
    ## Testing means of the samples
    ## This function chooses the appropriate test according to the results of the Shapiro-Wilks and F-test results
    ##
    ##  Usage:
    ##          output <- check_means((result, normality_results, var_h_results)
    ##
    ## Inputs:
    ##      - result :              a list containing processed results of the ARA experiment as generated by the
    ##                              data_formating_and_calc fucntion. Each element of the list correspond to one experiment
    ##                              in the tree and carries a data.frame containing all necessary informations from the analysis:
    ##                                  - condition_name
    ##                                  - sample_number
    ##                                  - the value of nmolC2H4_H_plant
    ##
    ##      - normality_results :   results generated by the fuction check_normality, which performs a Shapiro-Wilks test,
    ##                              a list in which each elements is related to an experiment and contins a list of two elements:
    ##                                  - p_values :        p.values calculated after a Shapiro-Wilks test.
    ##                                  - boolean_values :  determined according to threshold given as input of the check_normality function.
    ##
    ##      - var_h_results :       results generated by the fuction check_var_h, which verifies the variance homoscedasticity of each 
    ##                              pair of conditions within an experiment. This variable should be a list in which each elements 
    ##                              is related to an experiment and contins a list of two elements:
    ##                                  - p_values :        p.values calculated after a F-test.
    ##                                  - boolean_values :  determined according to threshold given as input of the check_var_h function.
    ##
    ## Ouput:
    ##      - means_analysis :      a list containing for each experiment: 
    ##                                  - test_performed : the name of the test used by the function to verify the means
    ##                              Then, the format of the list depends on the test performed:
    ##                              If the test was an ANOVA:
    ##                                  - 
    ##
    ##                              If the test was a one-way (welch corrected t-test):
    ##                                  -
    ##
    ##                              If the test was a Kruskal-Wallis
    ##                                  - 
    
    
    if (!require(car))
    {
        install.packages('car')
        
        library(car)
    }
    
    expeNbr <- length(names(result))
    
    for (i in 1:expeNbr)
    {
        condNames <- levels(as.factor(result[[i]]$condition_name))
        combNbr <- length(combn(length(condNames), 2))/2
        
        if (sum(normality_results[[i]]$boolean_result, na.rm = TRUE) == length(condNames))
        {
            if (sum(var_h_results[[i]]$boolean_results, na.rm = TRUE) == combNbr)
            {
                #Graph
                #par(mar=rep(2,4)) #marge des valeurs
                #boxplot(result[[i]]$nmolC2H4_H_plant ~ as.factor(result[[i]]$condition_name), ylab = "nmolC2H4.h.plant")
                
                # linear model for anova
                anova_model <- lm(result[[i]]$nmolC2H4_H_plant ~ as.factor(result[[i]]$condition_name), data = result[[i]] )
                
                # Graphics Verification of residues normality
                        # oldpar <- par(mar = rep(2,4), oma = rep(2,4), mfrow = c(2,2))
                pdf(file = paste(pathToExpeFolder, "/qqplot_anova_", gsub(" ", "_", names(result)[i]), ".pdf", sep = ""))
                plot(anova_model)
                        # par(oldpar)
                qqPlot(anova_model, simulate = TRUE, id.method = "y", id.n = 2, main = "Q-Q plot with confidence enveloppe")
                dev.off()
                
                # Verify residues normality
                res <- anova_model$residuals
                resNorm <- numeric()
                isResNorm <- logical()
                
                for (j in 1:length(condNames))
                {
                    resNorm[j] <- shapiro.test(res[which(result[[i]]$condition_name == condNames[j])])$p.value
                    if (resNorm[j] >= normalityThreshold)
                    {
                        isResNormal[j] <- TRUE          
                    }
                }
                
                # AnOVa
                Anova(anova_model, type = 2)
                
                postHocTukey <- TukeyHSD(aov(result[[i]]$nmolC2H4_H_plant ~ as.factor(result[[i]]$condition_name), data = result[[i]] ))
                plot(postHocTukey)
            }
            else
            {
                #kruslal-wallis (not normal) ou correction de welch (oneway.test ; var not H)
                    #kruskal.test(nmolArray[,i] ~ as.factor(allData[,"Name",i]))
                oneway.test(nmolArray[,i] ~ as.factor(allData[,"Name",i]), var.equal = FALSE)
            }
        }
        else
        {
            #kruslal-wallis
        }
    }
    
        # Detach package
    detach("package:car", unload = TRUE)
    ##    * 0.05     ** 0.01      *** 0.001
} # Ongoing

#---------------------------------------------------------------------------------------------------------------------------------------------------------------
                ########################################################################################################################
                ##                                FUNCTIONS RELATED TO DATA GRAPHICAL REPRESENTATION                                  ##
                ########################################################################################################################



#---------------------------------------------------------------------------------------------------------------------------------------------------------------
                ########################################################################################################################
                ##                                                   SAARA R FUNCTION                                                 ##
                ########################################################################################################################

saara <- function(pathToTemplate, pathToData, doStats = FALSE, statThresholdVar = 0.5, statThresholdNorm = 0.5, doGraphics = FALSE, colors = NA, splitFact = 5, vialVolume = 21, slope = 495)
{
    
}

#---------------------------------------------------------------------------------------------------------------------------------------------------------------

## Temp script

list_of_required_pckg <- data.frame(pckg = c("readxl", "xlsx", "car", "testit", "RInside"), 
                                    version = c("1.3.1", "0.6.1", "3.0-2", "0.9", "0.2.15"))

pathToExpeFolder <- "C:/Users/quentin.nicoud/Desktop/ARA test"
pathToExpeFolder <- "C:/Users/quent/Desktop/ARA test"

pathToTemplate <- "C:/Users/quentin.nicoud/Desktop/ARA test/temp.xlsx"
pathToTemplate <- "C:/Users/quent/Desktop/ARA test/temp.xlsx"

load_SAARA_packages(list_of_required_pckg)

extract <- data_extraction(pathToExpeFolder)
temp <- template_gen(pathToExpeFolder, pathToTemplate)
res <- data_formating_and_calc(extract, temp)

write_data(all_data, "F:/ARA2/results.xlsx")

result <- res
result[[1]] <- res[[1]][3:dim(res[[1]])[1],]

a <- check_normality(result)
normality_results <- a
b <- check_var_h(result)
var_h_results <- b

#---------------------------------------------------------------------------------------------------------------------------------------------------------------
#lt <- c(1:expeNbr)
#isLevene <- c(1:expeNbr)
#for (i in 1:expeNbr)
#{
##Leveene test
#  lt[i]<- leveneTest(nmolC2H4.h.plante[,i], as.factor(allData[,"Name",i]), center = median)["Pr(>F)"]
#  lt <- as.vector(lt)
#  if (lt[i]>=varHThreshold)
#  {
#    isLevene[i] <- TRUE
#  }
#  else {isLevene[i] <- FALSE}
#}